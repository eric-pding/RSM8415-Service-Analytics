{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3958ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install googletrans==3.1.0a0\n",
    "#!pip install vaderSentiment\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "from autocorrect import Speller\n",
    "from googletrans import Translator, constants\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "import numpy as geek\n",
    "import researchpy as rp\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report,confusion_matrix,roc_curve,auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d29024",
   "metadata": {},
   "source": [
    "# Reading a Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01021fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This reativly new temple\\'s a big hindu version of Disney Land.... Quite expensive entrance for a temple. I accept the religious dress code so i was not alowed to enter in shorts. I had to rent an Indian lunghi, male long dress. The guards was not polite, almost unfriendly. One lady guard was about to destroy my expensive camera wich i had to leave outside. Inside the temple it was to follow the marked way. Somwhere you was blessed by some holy man. But everywhere there was stands with different things for sale. Worst was the way out with an enormous market with holy souvenirs of different quality, most kitscy plastic things\" made in China\". The holy experience i had looked forward to have dissapeared in a commercial thing. All they wanted was my money.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_text = \"\"\"This reativly new temple's a big hindu version of Disney Land.... Quite expensive entrance for a temple. I accept the religious dress code so i was not alowed to enter in shorts. I had to rent an Indian lunghi, male long dress. The guards was not polite, almost unfriendly. One lady guard was about to destroy my expensive camera wich i had to leave outside. Inside the temple it was to follow the marked way. Somwhere you was blessed by some holy man. But everywhere there was stands with different things for sale. Worst was the way out with an enormous market with holy souvenirs of different quality, most kitscy plastic things\" made in China\". The holy experience i had looked forward to have dissapeared in a commercial thing. All they wanted was my money.\"\"\"\n",
    "original_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49d708",
   "metadata": {},
   "source": [
    "# Contraction Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c8dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary consisting of the contraction and the actual value\n",
    "Apos_dict={\"'s\":\" is\",\"n't\":\" not\",\"'m\":\" am\",\"'ll\":\" will\",\"'d\":\" would\",\"'ve\":\" have\",\"'re\":\" are\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efeefacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This reativly new temple is a big hindu version of Disney Land.... Quite expensive entrance for a temple. I accept the religious dress code so i was not alowed to enter in shorts. I had to rent an Indian lunghi, male long dress. The guards was not polite, almost unfriendly. One lady guard was about to destroy my expensive camera wich i had to leave outside. Inside the temple it was to follow the marked way. Somwhere you was blessed by some holy man. But everywhere there was stands with different things for sale. Worst was the way out with an enormous market with holy souvenirs of different quality, most kitscy plastic things\" made in China\". The holy experience i had looked forward to have dissapeared in a commercial thing. All they wanted was my money.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key,value in Apos_dict.items():\n",
    "    if key in original_text:\n",
    "        text_1=original_text.replace(key,value)\n",
    "text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d917406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contractions in the text:\n",
      "'s -> is\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nContractions in the text:\")\n",
    "for before, after in zip(word_tokenize(original_text), word_tokenize(text_1)):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97490f4",
   "metadata": {},
   "source": [
    "# Remove everything than word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c53c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This reativly new temple is a big hindu version of Disney Land Quite expensive entrance for a temple I accept the religious dress code so i was not alowed to enter in shorts I had to rent an Indian lunghi male long dress The guards was not polite almost unfriendly One lady guard was about to destroy my expensive camera wich i had to leave outside Inside the temple it was to follow the marked way Somwhere you was blessed by some holy man But everywhere there was stands with different things for sale Worst was the way out with an enormous market with holy souvenirs of different quality most kitscy plastic things made in China The holy experience i had looked forward to have dissapeared in a commercial thing All they wanted was my money '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_2 = re.sub('[^A-Za-z]+', ' ', text_1) \n",
    "text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2151ae53",
   "metadata": {},
   "source": [
    "# Uppercase to the lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "482bff0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this reativly new temple is a big hindu version of disney land quite expensive entrance for a temple i accept the religious dress code so i was not alowed to enter in shorts i had to rent an indian lunghi male long dress the guards was not polite almost unfriendly one lady guard was about to destroy my expensive camera wich i had to leave outside inside the temple it was to follow the marked way somwhere you was blessed by some holy man but everywhere there was stands with different things for sale worst was the way out with an enormous market with holy souvenirs of different quality most kitscy plastic things made in china the holy experience i had looked forward to have dissapeared in a commercial thing all they wanted was my money '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_3 = text_2.lower()\n",
    "text_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65ff6c",
   "metadata": {},
   "source": [
    "# Slang Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f5c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the file slang.txt\n",
    "path=\"C:\\\\Users\\\\Vahid\\\\OneDrive - University of Toronto\\\\MMA2023\\\\Jupyter files\\\\Data\\\\\"\n",
    "file=open(path+\"slang.txt\",\"r\")\n",
    "slang=file.read()\n",
    "\n",
    "#seperating each line present in the file\n",
    "slang=slang.split('\\n')\n",
    "\n",
    "# creating dictionary from slang.txt\n",
    "Slang_dict=dict()\n",
    "for line in slang:\n",
    "    temp=line.split(\"=\")\n",
    "    Slang_dict[temp[0]] = temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb1bbe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing slang with their actual words\n",
    "for word in word_tokenize(text_3):\n",
    "    if word in Slang_dict.keys():\n",
    "        print(word)\n",
    "        text_4=text_3.replace(word,Slang_dict[word])\n",
    "    else:\n",
    "        text_4=text_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8732d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Slangs corrected in the text:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSlangs corrected in the text:\")\n",
    "for before, after in zip(word_tokenize(text_3), word_tokenize(text_4)):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70c7ab",
   "metadata": {},
   "source": [
    "# Tokenization: Splitting text into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbf2daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = word_tokenize(text_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f078c2",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6796cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Import stopwords with nltk (predefined list of stop-words).\n",
    "stop = stopwords.words('english')\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2c851d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from text example\n",
    "text_6 = ' '.join([word for word in tokenized_text if word not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dc0dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words present in the text:  ['this', 'is', 'a', 'of', 'for', 'a', 'i', 'the', 'so', 'i', 'was', 'not', 'to', 'in', 'i', 'had', 'to', 'an', 'the', 'was', 'not', 'was', 'about', 'to', 'my', 'i', 'had', 'to', 'the', 'it', 'was', 'to', 'the', 'you', 'was', 'by', 'some', 'but', 'there', 'was', 'with', 'for', 'was', 'the', 'out', 'with', 'an', 'with', 'of', 'most', 'in', 'the', 'i', 'had', 'to', 'have', 'in', 'a', 'all', 'they', 'was', 'my']\n"
     ]
    }
   ],
   "source": [
    "# stop words identified in the text\n",
    "stop_words = [word for word in tokenized_text if word in stopwords.words('english')]\n",
    "print('stop words present in the text: ', stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89d766",
   "metadata": {},
   "source": [
    "# Tranlate to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a90a8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words that are different after translation to english:\n",
      "alowed -> allowed\n",
      "somwhere -> somewhere\n",
      "dissapeared -> disappeared\n"
     ]
    }
   ],
   "source": [
    "translated_text = GoogleTranslator(source='hi', target='en').translate(text_6)\n",
    "\n",
    "print(\"\\nWords that are different after translation to english:\")\n",
    "for before, after in zip(word_tokenize(text_6), word_tokenize(translated_text)):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8d53e",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35a1f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words that are different after stemming:\n",
      "reativly -> reativli\n",
      "temple -> templ\n",
      "quite -> quit\n",
      "expensive -> expens\n",
      "entrance -> entranc\n",
      "temple -> templ\n",
      "religious -> religi\n",
      "alowed -> alow\n",
      "shorts -> short\n",
      "guards -> guard\n",
      "polite -> polit\n",
      "unfriendly -> unfriendli\n",
      "lady -> ladi\n",
      "expensive -> expens\n",
      "leave -> leav\n",
      "outside -> outsid\n",
      "inside -> insid\n",
      "temple -> templ\n",
      "marked -> mark\n",
      "somwhere -> somwher\n",
      "blessed -> bless\n",
      "holy -> holi\n",
      "everywhere -> everywher\n",
      "stands -> stand\n",
      "different -> differ\n",
      "things -> thing\n",
      "enormous -> enorm\n",
      "holy -> holi\n",
      "souvenirs -> souvenir\n",
      "different -> differ\n",
      "quality -> qualiti\n",
      "kitscy -> kitsci\n",
      "things -> thing\n",
      "holy -> holi\n",
      "experience -> experi\n",
      "looked -> look\n",
      "dissapeared -> dissapear\n",
      "commercial -> commerci\n",
      "wanted -> want\n"
     ]
    }
   ],
   "source": [
    "# Initialize Porter Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming to each word\n",
    "text_7 = ' '.join([stemmer.stem(word) for word in word_tokenize(text_6)])\n",
    "\n",
    "# Print words that are different after stemming\n",
    "print(\"\\nWords that are different after stemming:\")\n",
    "for before, after in zip(word_tokenize(text_6), word_tokenize(text_7)):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a278e01a",
   "metadata": {},
   "source": [
    "# Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10850cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the spelling\n",
    "from textblob import TextBlob\n",
    "spell_correct = TextBlob(text_7).correct() \n",
    "text_8 = spell_correct.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "112cc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words that are different after lemmatization:\n",
      "templ -> temple\n",
      "disney -> dinner\n",
      "expens -> expense\n",
      "entranc -> entrance\n",
      "templ -> temple\n",
      "religi -> relief\n",
      "alow -> low\n",
      "lunghi -> lungs\n",
      "polit -> polite\n",
      "unfriendli -> unfriendly\n",
      "ladi -> laid\n",
      "expens -> expense\n",
      "wich -> with\n",
      "leav -> leave\n",
      "outsid -> outside\n",
      "insid -> inside\n",
      "templ -> temple\n",
      "somwher -> somewhere\n",
      "holi -> hold\n",
      "everywher -> everywhere\n",
      "enorm -> norm\n",
      "holi -> hold\n",
      "qualiti -> quality\n",
      "kitsci -> bitski\n",
      "holi -> hold\n",
      "experi -> expert\n",
      "dissapear -> disappear\n",
      "commerci -> commerce\n"
     ]
    }
   ],
   "source": [
    "# Print words that are different after spelling\n",
    "print(\"\\nWords that are different after lemmatization:\")\n",
    "for before, after in zip(word_tokenize(text_7), text_8):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc029bff",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e92e3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words that are different after lemmatization:\n",
      "lungs -> lung\n"
     ]
    }
   ],
   "source": [
    "# apply lemmatization to the text example\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# token after Lemmatization \n",
    "text_9 = [lemmatizer.lemmatize(word) for word in text_8]\n",
    "\n",
    "# Print words that are different after lemmatization\n",
    "print(\"\\nWords that are different after lemmatization:\")\n",
    "for before, after in zip(text_8, text_9):\n",
    "    if before != after:\n",
    "        print(f\"{before} -> {after}\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5ce19",
   "metadata": {},
   "source": [
    "# Create a wrapper to pre-process the given text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "599a19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contraction function to correct contract\n",
    "def preprocess_text_akram(text,contraction_dict=Apos_dict,slang_dict=Slang_dict,language='english'):\n",
    "    for key,value in Apos_dict.items():\n",
    "        if key in text:\n",
    "            text=text.replace(key,value)\n",
    "    \n",
    "    # change to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    ## Remove any symbols, spaces, punctuation, etc., then words.\n",
    "    text = re.sub('[^A-Za-z]+', ' ', text) \n",
    "    \n",
    "    #correct slang words\n",
    "    for word in word_tokenize(text_3):\n",
    "        if word in slang_dict.keys():\n",
    "            text=text.replace(word,slang_dict[word])           \n",
    "\n",
    "    return text\n",
    "\n",
    "# Define a custom tokenizer function\n",
    "def stemming_spelling_translated_lemma(text):\n",
    "    \n",
    "    translated_text = GoogleTranslator(source='hi', target='en').translate(text)\n",
    "    \n",
    "    tokens = word_tokenize(translated_text)\n",
    "        \n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = ' '.join([stemmer.stem(word) for word in tokens])\n",
    "    \n",
    "    spell_correct = TextBlob(stemmed_words).correct() \n",
    "    spelled_words = spell_correct.words\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in spelled_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Define a function to apply CountVectorizer to each element in the column\n",
    "def vectorize_akram(text,language='english',ngram_range_input=(1,1),binary_input=False):\n",
    "    stop_words = set(stopwords.words(language))\n",
    "\n",
    "    vectorizer = CountVectorizer( preprocessor=preprocess_text_akram,\n",
    "                                 binary=binary_input,\n",
    "                                 ngram_range=ngram_range_input, \n",
    "                                 tokenizer=stemming_spelling_lemmatize,\n",
    "                                 stop_words=stop_words )\n",
    "\n",
    "    # Transform text into a vector\n",
    "    vectorized_text = vectorizer.fit_transform([text])\n",
    "    return vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39c5008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_original_text = 'I went sightseeing in three people. Wide in unexpectedly, it is the most deep atmosphere in the way of local. Worship barefoot. People of shorts will visit dressed in cloth (rental). Yes Snack Corner, ants and souvenirs, is there a buffet restaurant, it has to some extent touristy, is only less expensive that the local-friendly, is heterogeneous. Worship time will take quite. Severe in 1 hour.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6f0cca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with custom preprocessing and tokenization\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae1cc8",
   "metadata": {},
   "source": [
    "# TF - IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99ff19b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TF-IDF Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wa</th>\n",
       "      <td>0.516835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worship</th>\n",
       "      <td>0.223554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>0.223554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>local</th>\n",
       "      <td>0.223554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>0.193813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>leave outside</th>\n",
       "      <td>0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long</th>\n",
       "      <td>0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long dress</th>\n",
       "      <td>0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>0.064604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>man</th>\n",
       "      <td>0.064604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>210 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               TF-IDF Weight\n",
       "wa                  0.516835\n",
       "worship             0.223554\n",
       "people              0.223554\n",
       "local               0.223554\n",
       "thing               0.193813\n",
       "...                      ...\n",
       "leave outside       0.064604\n",
       "long                0.064604\n",
       "long dress          0.064604\n",
       "look                0.064604\n",
       "man                 0.064604\n",
       "\n",
       "[210 rows x 1 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorization \n",
    "v = TfidfVectorizer(ngram_range=(1,2),\n",
    "                    preprocessor=preprocess_text_akram,\n",
    "                    tokenizer=stemming_spelling_translated_lemma,\n",
    "                    stop_words=stop_words )\n",
    "\n",
    "# creating dataframe from sparse matrix pf vectorization\n",
    "sparse_matrix_tfidf = v.fit_transform([original_text,second_original_text])\n",
    "tfidf_weight = sum(sparse_matrix_tfidf).toarray()[0]\n",
    "\n",
    "words_tfidf = pd.DataFrame(tfidf_weight, index=v.get_feature_names(), columns=['TF-IDF Weight'])\n",
    "words_tfidf.sort_values(by='TF-IDF Weight',inplace=True,ascending=False)\n",
    "words_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c12326",
   "metadata": {},
   "source": [
    "# Chi-Squre Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a65f4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accept</th>\n",
       "      <th>allow</th>\n",
       "      <th>almost</th>\n",
       "      <th>ant</th>\n",
       "      <th>atmosphere</th>\n",
       "      <th>barefoot</th>\n",
       "      <th>big</th>\n",
       "      <th>bitski</th>\n",
       "      <th>bless</th>\n",
       "      <th>buffet</th>\n",
       "      <th>...</th>\n",
       "      <th>visit</th>\n",
       "      <th>wa</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>went</th>\n",
       "      <th>wide</th>\n",
       "      <th>worship</th>\n",
       "      <th>worst</th>\n",
       "      <th>ye</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accept  allow  almost  ant  atmosphere  barefoot  big  bitski  bless  \\\n",
       "0       1      1       1    0           0         0    1       1      1   \n",
       "1       0      0       0    1           1         1    0       0      0   \n",
       "\n",
       "   buffet  ...  visit  wa  want  way  went  wide  worship  worst  ye  feedback  \n",
       "0       0  ...      0   1     1    1     0     0        0      1   0  positive  \n",
       "1       1  ...      1   0     0    1     1     1        1      0   1  negative  \n",
       "\n",
       "[2 rows x 94 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorization\n",
    "vectorizer = CountVectorizer(binary=True,\n",
    "                             ngram_range=(1,1),\n",
    "                             preprocessor=preprocess_text_akram, \n",
    "                             tokenizer=stemming_spelling_translated_lemma,\n",
    "                             stop_words=stop_words )\n",
    "\n",
    "sparse_matrix = vectorizer.fit_transform([original_text,second_original_text])\n",
    "X_chs = pd.DataFrame(data = sparse_matrix.toarray() ,columns=vectorizer.get_feature_names_out())\n",
    "X_chs['feedback']=['positive','negative']\n",
    "X_chs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbf99f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_chs['feedback']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1b38c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab, test_results, expected = rp.crosstab(X_chs['big'].astype(object), \n",
    "                                               y,\n",
    "                                               test= \"chi-square\",\n",
    "                                               expected_freqs= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5498d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Chi-Square table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">feedback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feedback</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feedback             \n",
       "feedback negative positive All\n",
       "big                           \n",
       "0               1        0   1\n",
       "1               0        1   1\n",
       "All             1        1   2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Observed Chi-Square table:')\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d698c1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Chi-Square table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">feedback</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feedback</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         feedback         \n",
       "feedback negative positive\n",
       "big                       \n",
       "0             0.5      0.5\n",
       "1             0.5      0.5"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Expected Chi-Square table:')\n",
    "expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bbc7f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chi-square test</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pearson Chi-square ( 1.0) =</td>\n",
       "      <td>2.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p-value =</td>\n",
       "      <td>0.1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cramer's phi =</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Chi-square test  results\n",
       "0  Pearson Chi-square ( 1.0) =    2.0000\n",
       "1                    p-value =    0.1573\n",
       "2               Cramer's phi =    1.0000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b2db3",
   "metadata": {},
   "source": [
    "# Unsupervised Learning -  AFINN Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4bfe6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" A unique temple happy unique sad\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7f39df0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :  0.0\n",
      "\n",
      "\n",
      "unique :  0.0\n",
      "\n",
      "\n",
      "temple :  0.0\n",
      "\n",
      "\n",
      "happy :  3.0\n",
      "\n",
      "\n",
      "unique :  0.0\n",
      "\n",
      "\n",
      "sad :  -2.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "text_tokenized = word_tokenize(text)\n",
    "\n",
    "for i in text_tokenized:\n",
    "    affin_score = Afinn().score(i)\n",
    "    print(i, ': ', affin_score)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9de5ef38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affin score of all text \n",
    "Afinn().score(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
